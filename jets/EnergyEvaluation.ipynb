{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Evaluation\n",
    "\n",
    "This notebook just evaluates our classification + energy regressions on our jet data. The same processes are done in `JetClustering.ipynb`, but here we leave out all the stuff with jet clustering (which is only interesting once we have good performance on the topo-clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "\n",
    "# Flag for skipping file preparation, if it has already been done.\n",
    "skip_scores = False\n",
    "\n",
    "# Debug: Uses only one input file, which will speed things up.\n",
    "debug = True\n",
    "\n",
    "# classification threshold -- above = charged, below = neutral\n",
    "classification_threshold = 0.6\n",
    "\n",
    "# explicitly set the directory where the classifier is (for now we only deploy 1 classification at a time)\n",
    "classification_dir = path_prefix + 'classifier/Models/pion'\n",
    "\n",
    "# give a list of directories for regressions -- we may try multiple models\n",
    "regression_dirs = ['pion']\n",
    "regression_dirs = [path_prefix + 'regression/Models/' + x for x in regression_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "# Imports - generic stuff\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import ROOT as rt\n",
    "import uproot as ur # uproot for accessing ROOT files quickly (and in a Pythonic way)\n",
    "import sys, os, glob, uuid # glob for searching for files, uuid for random strings to name ROOT objects and avoid collisions\n",
    "import subprocess as sub\n",
    "from numba import jit\n",
    "from pathlib import Path\n",
    "from IPython.utils import io # For suppressing some print statements from functions.\n",
    "\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import ml_util as mu # for passing calo images to regression networks\n",
    "from util import qol_util as qu # for progress bar\n",
    "from util import jet_util as ju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display our plots, let's get a dark style that will look nice in presentations (and JupyterLab in dark mode).\n",
    "dark_style = qu.PlotStyle('dark')\n",
    "light_style = qu.PlotStyle('light')\n",
    "plot_style = dark_style\n",
    "plot_style.SetStyle() # sets style for plots - still need to adjust legends, paves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import `tensorflow` (and some of its `keras` stuff), as well as some stuff from `sklearn` for neural network I/O scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup for TensorFlow and Keras.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))\n",
    "\n",
    "# Dictionary for storing all our neural network models that will be evaluated\n",
    "network_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a whole bunch of paths. We use `source = pion` by default, whereby we use networks trained on the single-pion data. We can alternatively use `source = jet` to use our \"facsimile jet training data\" -- a subset of jet data where we have tried to match topo-clusters to pions -- but this is not fully implemented yet. For example, this workflow explicitly re-derives the network scalers using the `pion` data, so to use the `jet` data we will have to modify that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = path_prefix + 'data/jet'\n",
    "fj_dir   = path_prefix + '/setup/fastjet/fastjet-install/lib/python3.8/site-packages'\n",
    "plot_dir = path_prefix + 'jets/clusterPlots/'\n",
    "\n",
    "try: os.makedirs(plot_dir)\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Calorimeter meta-data -----\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "nlayers = len(layers)\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "assert(len(len_phi) == nlayers)\n",
    "assert(len(len_eta) == nlayers)\n",
    "meta_data = {\n",
    "    layers[i]:{\n",
    "        'cell_size':(cell_size_eta[i],cell_size_phi[i]),\n",
    "        'dimensions':(len_eta[i],len_phi[i])\n",
    "    }\n",
    "    for i in range(nlayers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flat classification models... \n",
      "\tLoading EMB1... Done.\n",
      "\tLoading EMB2... Done.\n",
      "\tLoading EMB3... Done.\n",
      "\tLoading TileBar0... Done.\n",
      "\tLoading TileBar1... Done.\n",
      "\tLoading TileBar2... Done.\n",
      "Loading simple combo classification model... Done.\n",
      "Loading 1 sets of energy regression models.\n",
      "\tLoading charged-pion energy regression model #1... Done.\n",
      "\tLoading neutral-pion energy regression model #1... Done.\n"
     ]
    }
   ],
   "source": [
    "# flat classifiers\n",
    "print('Loading flat classification models... ')\n",
    "flat_model_files = glob.glob(classification_dir + '/flat/' + '*.h5')\n",
    "flat_model_files.sort()\n",
    "flat_model_names = []\n",
    "for model in flat_model_files:\n",
    "    model_name = model.split('model_')[-1].split('_flat')[0]\n",
    "    print('\\tLoading ' + model_name + '... ',end='')\n",
    "    flat_model_names.append(model_name)\n",
    "    network_models[model_name] = tf.keras.models.load_model(model)\n",
    "    print('Done.')\n",
    "\n",
    "# combo classifier\n",
    "print('Loading simple combo classification model... ',end='')\n",
    "combo_model_file = classification_dir + '/simple/' + 'model_simple_do20.h5'\n",
    "network_models['combo'] = tf.keras.models.load_model(combo_model_file)\n",
    "print('Done.')\n",
    "\n",
    "# energy regression networks\n",
    "charged_keys = {}\n",
    "neutral_keys = {}\n",
    "\n",
    "n_regressions = len(regression_dirs)\n",
    "print('Loading {} sets of energy regression models.'.format(n_regressions))\n",
    "\n",
    "for i in range(n_regressions):\n",
    "    charged_key = 'e_charged' + '_' + str(i).zfill(2)\n",
    "    neutral_key = 'e_neutral' + '_' + str(i).zfill(2)\n",
    "    \n",
    "    charged_keys[regression_dirs[i]] = charged_key\n",
    "    neutral_keys[regression_dirs[i]] = neutral_key\n",
    "    \n",
    "    # we will look for h5 files -- one with \"charged\" in the name, one with \"neutral\" in the name\n",
    "    modelfiles = glob.glob(regression_dirs[i] + '/*.h5')\n",
    "    charged_model_file = 0\n",
    "    neutral_model_file = 0\n",
    "    for file in modelfiles:\n",
    "        if('charged') in file: charged_model_file = file\n",
    "        elif('neutral') in file: neutral_model_file = file\n",
    "    assert(charged_model_file !=0)\n",
    "    assert(neutral_model_file !=0)\n",
    "\n",
    "    print('\\tLoading charged-pion energy regression model #{}... '.format(i+1),end='')\n",
    "    network_models[charged_key] = tf.keras.models.load_model(charged_model_file)\n",
    "    print('Done.')\n",
    "    \n",
    "    print('\\tLoading neutral-pion energy regression model #{}... '.format(i+1),end='')\n",
    "    network_models[neutral_key] = tf.keras.models.load_model(neutral_model_file)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a \"local\" copy of the jet data. We will only copy over certain branches, and we will skip any files that don't contain an `eventTree` in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying data files: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "# our \"local\" data dir, where we create modified data files\n",
    "jet_data_dir = path_prefix + 'jets/cluster_data'\n",
    "Path(jet_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if(skip_scores):\n",
    "    data_filenames = glob.glob(jet_data_dir + '/*.root')\n",
    "    \n",
    "    # debugging - take only one file, for speed\n",
    "    if(debug): data_filenames = [data_filenames[0]]\n",
    "    \n",
    "else:\n",
    "    data_filenames = glob.glob(data_dir + '/' + '*.root')\n",
    "\n",
    "    # debugging - lets us use a single file to speed stuff up a lot.\n",
    "    if(debug): data_filenames = [data_dir + '/' + 'user.angerami.21685345.OutputStream._000062.root']\n",
    "\n",
    "    # Get the original data.\n",
    "    files = {name:rt.TFile(name,'READ') for name in data_filenames}\n",
    "\n",
    "    # Some data files might be missing an EventTree.\n",
    "    # For now, we will skip these because our methods count on an existing EventTree.\n",
    "    delete_keys = []\n",
    "    for key, val in files.items():\n",
    "        file_keys = [x.GetName() for x in val.GetListOfKeys()]\n",
    "        if('ClusterTree' not in file_keys or 'EventTree' not in file_keys):\n",
    "            delete_keys.append(key)\n",
    "\n",
    "    for key in delete_keys: \n",
    "        print('Ignoring file:',key,'(no EventTree/ClusterTree found).')\n",
    "        del files[key]\n",
    "\n",
    "    # now we make a local copy of the files in the jet_data_dir, keeping only certain branches\n",
    "    active_branches = {}\n",
    "    active_branches['cluster'] = [\n",
    "        'runNumber',\n",
    "        'eventNumber',\n",
    "        'truthE',\n",
    "        'truthPt',\n",
    "        'truthEta',\n",
    "        'truthPhi',\n",
    "        'clusterIndex',\n",
    "        'nCluster',\n",
    "        'clusterE',\n",
    "        'clusterECalib',\n",
    "        'clusterPt',\n",
    "        'clusterEta',\n",
    "        'clusterPhi',\n",
    "        'cluster_nCells',\n",
    "        'cluster_ENG_CALIB_TOT',\n",
    "        'EMB1',\n",
    "        'EMB2',\n",
    "        'EMB3',\n",
    "        'TileBar0',\n",
    "        'TileBar1',\n",
    "        'TileBar2'\n",
    "    ]\n",
    "\n",
    "    tree_names = {'cluster':'ClusterTree'}\n",
    "    data_filenames = []\n",
    "\n",
    "    l = len(files.keys())\n",
    "    i = 0\n",
    "    qu.printProgressBarColor(i, l, prefix='Copying data files:', suffix='Complete', length=50)\n",
    "\n",
    "    for path, tfile in files.items():\n",
    "        filename_new = jet_data_dir + '/' + path.split('/')[-1]\n",
    "        old_trees = {x:tfile.Get(tree_names[x]) for x in tree_names.keys()}\n",
    "    \n",
    "        for key, tree in old_trees.items():\n",
    "            tree.SetBranchStatus('*',0)\n",
    "            for bname in active_branches[key]: tree.SetBranchStatus(bname,1)\n",
    "    \n",
    "        tfile_new = rt.TFile(filename_new,'RECREATE')\n",
    "        new_trees = {x:old_trees[x].CloneTree() for x in old_trees.keys()}\n",
    "        tfile_new.Write()\n",
    "        data_filenames.append(filename_new)\n",
    "        i += 1\n",
    "        qu.printProgressBarColor(i, l, prefix='Copying data files:', suffix='Complete', length=50)\n",
    "        del old_trees\n",
    "        del new_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the files & trees with uproot\n",
    "tree_names = {'cluster':'ClusterTree'}\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides our models and the data, we also need the *scalers* associated with the regression models. We will apply these to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jl\n",
    "\n",
    "# Fetch the scalers associated with the regression models\n",
    "scaler_file = 'scalers.save' # name is always the same (for now)\n",
    "scalers = {rd:jl.load(rd + '/' + scaler_file) for rd in regression_dirs}\n",
    "keys = ['e','cal','eta']\n",
    "for key, scaler_dict in scalers.items():\n",
    "    for key2 in keys:\n",
    "        scaler_dict[key2]['charged'] = scaler_dict[key2]['pp']\n",
    "        scaler_dict[key2]['neutral'] = scaler_dict[key2]['p0']\n",
    "        del scaler_dict[key2]['pp']\n",
    "        del scaler_dict[key2]['p0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting network outputs for all clusters\n",
    "\n",
    "Now we will loop over our data files, and get network scores (classification and predicted energies) for all clusters. Note that the latter involves *scaling* of the data, which we will achieve using the scalers that we extracted from the training data above.\n",
    "\n",
    "This isn't the most notebook-esque code, as we're preparing a bunch of inputs *within* the big for loop below (and not saving them or printing them) but it should avoid \"out of memory\" issues: As we are dealing with a large amount of data, preparing all the data in memory before operating on it will result in very high memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /local/home/jano/ml4pions/LCStudies/jets/../jets/cluster_data/user.angerami.21685345.OutputStream._000062.root\n",
      "\tPrepping calo images...\n",
      "\tPrepping extra inputs...\n",
      "\tCalculating network outputs...\n",
      "\t\tClassification... Done.\n",
      "\t\tRegression... \t\t\tLoading regression #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-960dd9edb2a6>:78: RuntimeWarning: overflow encountered in exp\n",
      "  model_scores[name] = np.exp(scalers[reg]['cal']['neutral'].inverse_transform(model.predict(regression_input['neutral'])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving network scores to tree ScoreTree... Done.\n"
     ]
    }
   ],
   "source": [
    "# branch buffer for filling our score trees\n",
    "    # make our branch buffer\n",
    "branch_buffer = {'charged_likelihood_combo': np.zeros(1,dtype=np.dtype('f8'))}\n",
    "\n",
    "for key in charged_keys.keys():\n",
    "    branch_buffer[charged_keys[key]] = np.zeros(1,dtype=np.dtype('f8'))\n",
    "    branch_buffer[neutral_keys[key]] = np.zeros(1,dtype=np.dtype('f8'))\n",
    "\n",
    "# Name for the tree that will contain network scores.\n",
    "tree_name = 'ScoreTree'\n",
    "\n",
    "for dfile, trees in ur_trees.items():\n",
    "    \n",
    "    if(skip_scores): \n",
    "        # Explicitly check if ScoreTree is present, otherwise we recompute.\n",
    "        # Useful if score computation was previously interrupted.\n",
    "        file_keys = [str(x,'utf-8') for x in list(ur.open(dfile).keys())]\n",
    "        skip = [tree_name in fkey for fkey in file_keys]\n",
    "        if(True in skip): continue    \n",
    "        \n",
    "    print ('File:',dfile)\n",
    "    # Prepare the calo images.\n",
    "    print('\\tPrepping calo images...')\n",
    "    calo_images = {}\n",
    "    for layer in layers:\n",
    "        calo_images[layer] = mu.setupCells(trees['cluster'],layer)\n",
    "    combined_images = np.concatenate(tuple([calo_images[layer] for layer in layers]), axis=1)\n",
    "\n",
    "    # Prepare some extra combined input for the energy regressions.\n",
    "    print('\\tPrepping extra inputs...')\n",
    "    \n",
    "    e = trees['cluster'].array('clusterE')\n",
    "    e_calib = trees['cluster'].array('cluster_ENG_CALIB_TOT')\n",
    "    eta = trees['cluster'].array('clusterEta')\n",
    "    \n",
    "    # cleaning for e_calib (empirically needed for e_calib to remove values that are too small)\n",
    "    epsilon = 1.0e-12 #1.0e-12 # TODO: Should I set this to energy_cut as defined in regression training? Would that make sense?\n",
    "    e_calib = np.where(e_calib < epsilon, epsilon, e_calib)\n",
    "    \n",
    "    s_combined,scaler_combined = mu.standardCells(combined_images, layers) # Note: scaler_combined is unused\n",
    "    \n",
    "    # find network scores\n",
    "    print('\\tCalculating network outputs...')\n",
    "    model_scores = {}\n",
    "    \n",
    "    print('\\t\\tClassification... ', end='')\n",
    "    # 1) flat networks\n",
    "    for layer in flat_model_names:\n",
    "        model = network_models[layer]\n",
    "        model_scores[layer] = model.predict(calo_images[layer])[:,1] # [:,1] based on Max's code, this is input to combo network. Likelihood of being charged (vs. neutral)\n",
    "    \n",
    "    # 2) combo network\n",
    "    name = 'combo'\n",
    "    model = network_models[name]\n",
    "    input_scores = np.column_stack([model_scores[layer] for layer in layers])\n",
    "    model_scores[name] = model.predict(input_scores)[:,1] # likelihood of being charged pion (versus neutral pion)\n",
    "    print('Done.')\n",
    "    \n",
    "    print('\\t\\tRegression... ', end='')\n",
    "    # 3) energy regression networks\n",
    "    \n",
    "    for i, reg in enumerate(charged_keys.keys()):\n",
    "        print('\\t\\t\\tLoading regression #{}'.format(i+1))\n",
    "        regression_input = {}\n",
    "        for key in scalers[reg]['cal'].keys():\n",
    "            s_logE = scalers[reg]['e'  ][key].transform(np.log(e).reshape(-1,1))\n",
    "            s_eta  = scalers[reg]['eta'][key].transform(eta.reshape(-1,1))\n",
    "            regression_input[key] = np.column_stack((s_logE, s_eta,s_combined))\n",
    "        \n",
    "        # charged model\n",
    "        name = charged_keys[reg]\n",
    "        model = network_models[name]\n",
    "        model_scores[name] = np.exp(scalers[reg]['cal']['charged'].inverse_transform(model.predict(regression_input['charged'])))\n",
    "        \n",
    "        # neutral model\n",
    "        name = neutral_keys[reg]\n",
    "        model = network_models[name]\n",
    "        model_scores[name] = np.exp(scalers[reg]['cal']['neutral'].inverse_transform(model.predict(regression_input['neutral'])))\n",
    "    \n",
    "    # Now we should save these scores to a new tree.\n",
    "    f = rt.TFile(dfile, 'UPDATE')\n",
    "    t = rt.TTree(tree_name, tree_name)\n",
    "    \n",
    "    print('Saving network scores to tree ' + tree_name + '... ',end='')    \n",
    "    # --- Setup the branches using our buffer. This is a rather general/flexible code block. ---\n",
    "    branches = {}\n",
    "    for bname, val in branch_buffer.items():\n",
    "        descriptor = bname\n",
    "        bshape = val.shape\n",
    "        if(bshape != (1,)):\n",
    "            for i in range(len(bshape)):\n",
    "                descriptor += '[' + str(bshape[i]) + ']'\n",
    "        descriptor += '/'\n",
    "        if(val.dtype == np.dtype('i2')): descriptor += 'S'\n",
    "        elif(val.dtype == np.dtype('i4')): descriptor += 'I'\n",
    "        elif(val.dtype == np.dtype('i8')): descriptor += 'L'\n",
    "        elif(val.dtype == np.dtype('f4')): descriptor += 'F'\n",
    "        elif(val.dtype == np.dtype('f8')): descriptor += 'D'\n",
    "        else:\n",
    "            print('Warning, setup issue for branch: ', key, '. Skipping.')\n",
    "            continue\n",
    "        branches[bname] = t.Branch(bname,val,descriptor)\n",
    "    \n",
    "    # Fill the model score tree, and save it to the local data file.\n",
    "    nentries = model_scores['combo'].shape[0]\n",
    "    for i in range(nentries):\n",
    "        branch_buffer['charged_likelihood_combo'][0] = model_scores['combo'][i]\n",
    "        for key in charged_keys.keys():\n",
    "            branch_buffer[charged_keys[key]][0] = model_scores[charged_keys[key]][i]\n",
    "            branch_buffer[neutral_keys[key]][0] = model_scores[neutral_keys[key]][i]\n",
    "        t.Fill()\n",
    "    \n",
    "    t.Write(tree_name, rt.TObject.kOverwrite)\n",
    "    f.Close()\n",
    "    print('Done.')\n",
    "    \n",
    "tree_names['score'] = tree_name\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to jet clustering, we can already check to see if our energy regressions seem sensible. Let's make distributions of:\n",
    "- The classification score\n",
    "- Each regressed energy, for **all** clusters (i.e. charged and neutral energy regressions for all clusters regardless of their classifications)\n",
    "- Regressed energy / reco energy, where we choose the regressed energy for each cluster based on its classification score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from energy_ratio import EnergyRatioHist, EnergyRatioHist2D\n",
    "rt.gStyle.SetOptStat(0)\n",
    "alpha=0.5\n",
    "\n",
    "# parameters for our energy ratio histograms\n",
    "nbins = 10000\n",
    "xmin = 1.0e-3\n",
    "xmax = 1.0e2\n",
    "nsteps = 25\n",
    "class_min = 0.3\n",
    "class_max = 0.8\n",
    "\n",
    "plot_size = (1000,800)\n",
    "n_plots = 3\n",
    "c = rt.TCanvas(str(uuid.uuid4()),'network checks',plot_size[0],n_plots * plot_size[1])\n",
    "c.Divide(1,n_plots)\n",
    "\n",
    "# classification scores\n",
    "class_hist = rt.TH1F(str(uuid.uuid4()), 'Classification score (charged likelihood);Score;Count',100,0.,1.)\n",
    "for dfile, trees in ur_trees.items():\n",
    "    for score in ur_trees[dfile]['score'].array('charged_likelihood_combo'): class_hist.Fill(score)\n",
    "class_hist.SetFillColorAlpha(rt.kGreen,alpha)\n",
    "class_hist.SetLineColorAlpha(rt.kGreen,alpha)\n",
    "c.cd(1)\n",
    "class_hist.Draw('HIST')\n",
    "rt.gPad.SetLogy()\n",
    "\n",
    "# regressed energy (most likely) / calibration hits\n",
    "energy_ratio_hist = EnergyRatioHist(ur_trees, charged_key ='e_charged_00',neutral_key ='e_neutral_00',classification_threshold=classification_threshold)\n",
    "c.cd(2)\n",
    "energy_ratio_hist.Draw('HIST')\n",
    "rt.gPad.SetLogx()\n",
    "rt.gPad.SetLogy()\n",
    "\n",
    "# regressed energy / calibration hits, as a function of our classification threshold\n",
    "c.cd(3)\n",
    "energy_ratio_2d = EnergyRatioHist2D(ur_trees, charged_key='e_charged_00', neutral_key = 'e_neutral_00', class_min=class_min, class_max=class_max, nsteps=nsteps, nbins=nbins, xmin=xmin, xmax=xmax)\n",
    "energy_ratio_2d.SetBarWidth(0.4)\n",
    "energy_ratio_2d.SetLineColor(plot_style.curve)\n",
    "energy_ratio_2d.SetFillColor(plot_style.main)\n",
    "energy_ratio_2d.Draw('CANDLEY3')\n",
    "rt.gPad.SetLogx()\n",
    "rt.gPad.SetRightMargin(0.2)\n",
    "\n",
    "# Let's keep track of how many clusters have cluster_ENG_CALIB_TOT = 0.\n",
    "n_tot = 0\n",
    "zero_energies = 0\n",
    "for dfile, trees in ur_trees.items():\n",
    "    eng_calib_tot = trees['cluster'].array('cluster_ENG_CALIB_TOT')\n",
    "    n_tot += len(eng_calib_tot)\n",
    "    zero_energies += np.sum(eng_calib_tot == 0.)\n",
    "\n",
    "print('Number of clusters with ENG_CALIB_TOT == 0: {val1:.2e} ({val2:.2f}% of clusters)'.format(val1 = zero_energies, val2 = 100. * zero_energies / n_tot))\n",
    "\n",
    "c.SaveAs(plot_dir + '/' + 'cluster_plots.png')\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparing performance between different network configurations, it will be helpful to save the histograms of predicted/true energy to `ROOT` files. This way, we can easily combine a bunch of them later for a rigorous comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
